# Evo
## EvoStructâ€‘CoT

> **Evolving Structural Representations for Robust Chainâ€‘ofâ€‘Thought Prompting in Large Language Models**

[![License: Unlicense](https://img.shields.io/badge/license-Unlicense-blue.svg)](LICENSE)Â Â [![Build](https://github.com/your-org/EvoStruct-CoT/actions/workflows/ci.yml/badge.svg)](https://github.com/your-org/EvoStruct-CoT/actions/workflows/ci.yml)

---

## 1Â |Â Abstract
*EvoStructâ€‘CoT* is an **openâ€‘science project** that studies whether *geneticallyâ€‘optimised structural graphs of reasoning* can act as higherâ€‘quality scaffolds for Chainâ€‘ofâ€‘Thought (CoT) prompting.  
We fuse three ideas:

1. **Structure induction via LLMs**Â to obtain an initial *Graphâ€‘ofâ€‘Thought* (GoT)Â [6].
2. **Genetic programming (GP)**, inspired by evolutionary prompt searchÂ [5], to optimise GoTs under a multiâ€‘objective fitness function (accuracy, coherence, cost).
3. **LLMâ€‘toâ€‘LLM translation** to render the optimised GoT into a naturalâ€‘language CoT consumable by any target LLM.

Our hypothesis: these *Evolved CoTs* will surpass Zeroâ€‘Shot CoTÂ [1], Fewâ€‘Shot CoT, and hierarchical prompting baselines such as Treeâ€‘ofâ€‘ThoughtÂ [2], Leastâ€‘toâ€‘MostÂ [3] and ReAct.

---

## 2Â |Â Scientific Motivation
Large Language Models excel at pattern completion yet still fail on deliberate, multiâ€‘step reasoning.  
Typical remediesâ€”manual exemplars or *thinkâ€‘stepâ€‘byâ€‘step* heuristicsâ€”suffer from brittleness and limited coverage.  
We argue that **reasoning should be an explicit, evolvable artefact**: by treating the reasoning structure as a firstâ€‘class citizen, we can (i) *measure* its quality, (ii) *optimise* it algorithmically, and (iii) *audit* it postâ€‘hoc.

---

## 3Â |Â Novel Contributions
* **GoTÂ Schema**Â â€“Â A typeâ€‘annotated, JSONâ€‘serialisable representation of reasoning steps (Â§Â `evo_struct/schema.py`).
* **EvoPrompt Engine**Â â€“Â A DEAPâ€‘based GP engine for evolving GoTs with Pareto optimisation over **Accuracy**, **Selfâ€‘Consistency**Â [7] and **Tokenâ€‘Cost**.
* **Translatorâ€‘LM**Â â€“Â A modular component that converts any GoT into a coherent CoT using templating or an auxiliary LLM.
* **Costâ€‘Aware Benchmarking**Â â€“Â Every experiment logs \$â€‘perâ€‘correctâ€‘answer, enabling realistic tradeâ€‘off analysis.

---

## 4Â |Â Background & Related Work
| Topic | Key Papers |
|-------|------------|
| Chainâ€‘ofâ€‘Thought Prompting | Wei *etÂ al.*Â (2022)Â [1] |
| Hierarchical Reasoning | Yao *etÂ al.*Â â€“Â Treeâ€‘ofâ€‘ThoughtÂ (2023)Â [2]; Arora *etÂ al.*Â â€“Â Graphâ€‘ofâ€‘ThoughtÂ (2025)Â [6] |
| Prompt Evolution | Guo *etÂ al.*Â â€“Â PromptBreederÂ (2023)Â [5] |
| Decomposition & Planning | Zhou *etÂ al.*Â â€“Â Leastâ€‘toâ€‘MostÂ (2022)Â [3]; Gao *etÂ al.*Â â€“Â PALÂ (2023)Â [4] |
| Selfâ€‘Consistency | Zhang *etÂ al.*Â (2023)Â [7] |

Our work is the **first to unify** LLMâ€‘based structure induction **and** GPâ€‘driven optimisation within a single, reproducible framework.

---

## 5Â |Â Repository Layout
```text
.
â”œâ”€â”€ evo_struct            # Core GP engine & GoT utilities
â”œâ”€â”€ translators           # GoT â†’ CoT converters
â”œâ”€â”€ baselines             # Zeroâ€‘Shot, Fewâ€‘Shot, ToT, L2M, ReAct
â”œâ”€â”€ datasets              # Download & preprocessing scripts
â”œâ”€â”€ experiments           # Hydraâ€‘driven runner
â”œâ”€â”€ evaluation            # Metrics & statistical tests
â”œâ”€â”€ docs                  # Sphinx documentation
â””â”€â”€ ci                    # GitHub Actions workflows
```

---

## 6Â |Â Quickâ€‘Start
```bash
# Clone & install (Poetry + preâ€‘commit)
$ git clone https://github.com/yourâ€‘org/EvoStructâ€‘CoT.git && cd EvoStructâ€‘CoT
$ make install

# Smoke test on 10 GSM8K problems
$ make demo

# Full experiment with custom params
$ python -m experiments.run +exp=gsm8k_evo_struct \
      +model=gpt4 +gp.population=64 +gp.generations=30
```

---

## 7Â |Â Datasets
| Benchmark  | Domain                   | License      |
|------------|--------------------------|--------------|
| GSM8K      | Gradeâ€‘school mathematics | CCÂ BYâ€‘SAÂ 4.0 |
| StrategyQA | Multiâ€‘hop commonsense    | MIT          |
| LogiQA     | Logical reading          | CCÂ BYâ€‘SAÂ 4.0 |

Run `make data` to fetch and checksum all corpora.

---

## 8Â |Â Reproducing the Paper
```bash
# 1. Ensure API keys / local models are configured (docs/models.md)
# 2. Execute the three canonical experiments
$ make paper
# 3. Results (CSV + plots) will appear under artifacts/paperâ€‘results/
```

---

## 9Â |Â Building in Public
* ðŸ—“ **Weekly Research Logs** â€“ posted under *GitHubÂ DiscussionsÂ â–¸ #buildâ€‘log* every Friday.
* ðŸ“Š **Project Board** â€“ Kanban tracking of TODO.md.
* ðŸ”– **Public Milestones** â€“ semantic versioning (`v0.1`, `v0.2â€‘beta`, `v1.0â€‘paper`).

---

## 10Â |Â Contributing
Pull requests that introduce new baselines, improve evaluation, or harden infrastructure are welcome.  
Please read `CONTRIBUTING.md` and sign the CLA.

---

## 11Â |Â Citation
```bibtex
@misc{touma2025evostruct,
  title  = {EvoStructâ€‘CoT: Evolving Structural Representations for Chainâ€‘ofâ€‘Thought Prompting},
  author = {Touma, Karim},
  year   = {2025},
  howpublished = {GitHub repository},
  url    = {https://github.com/karimtouma/evo}
}
```

---

## 12Â |Â License
This project is released under **The Unlicense** â€“Â see `LICENSE` for details.  
*"This is free and unencumbered software released into the public domain."*

---

## 13Â |Â Bibliography
[1] WeiÂ etÂ al., *Chainâ€‘ofâ€‘Thought Prompting Elicits Reasoning in Large Language Models*, 2022.  
[2] YaoÂ etÂ al., *Treeâ€‘ofâ€‘Thought Deliberate Reasoning*, 2023.  
[3] ZhouÂ etÂ al., *Leastâ€‘toâ€‘Most Prompting Enables Complex Reasoning in LLMs*, 2022.  
[4] GaoÂ etÂ al., *PAL: Programâ€‘Aided Language Models*, 2023.  
[5] GuoÂ etÂ al., *PromptBreeder: Gradientâ€‘Free Prompt Evolution for LLMs*, 2023.  
[6] AroraÂ etÂ al., *Graphâ€‘ofâ€‘Thought: Unifying Structured Reasoning for LLMs*, 2025.  
[7] ZhangÂ etÂ al., *Automatic Chainâ€‘ofâ€‘Thought Prompting via Selfâ€‘Consistency*, 2023.

---
*Maintainers:* [@karimtouma](https://github.com/karimtouma).

